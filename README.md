# ðŸš€ Big Data Integration Project: Job Market Analytics

## ðŸ“‹ Tá»•ng quan Dá»± Ã¡n

Dá»± Ã¡n tÃ­ch há»£p vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n tá»« 3 nguá»“n viá»‡c lÃ m khÃ¡c nhau Ä‘á»ƒ táº¡o ra insights vá» thá»‹ trÆ°á»ng lao Ä‘á»™ng toÃ n cáº§u.

### ðŸŽ¯ Má»¥c tiÃªu
- TÃ­ch há»£p dá»¯ liá»‡u tá»« 3 nguá»“n: CareerLink, Joboko, TopCV
- PhÃ¢n tÃ­ch xu hÆ°á»›ng thá»‹ trÆ°á»ng viá»‡c lÃ m
- Dá»± Ä‘oÃ¡n má»©c lÆ°Æ¡ng vÃ  yÃªu cáº§u ká»¹ nÄƒng
- Táº¡o dashboard trá»±c quan hÃ³a dá»¯ liá»‡u

## ðŸ“Š Dataset

| Dataset | Nguá»“n | KÃ­ch thÆ°á»›c | MÃ´ táº£ |
|---------|-------|------------|-------|
| `data_careerlink.json` | CareerLink | ~15,772 records | Viá»‡c lÃ m IT táº¡i Viá»‡t Nam |
| `data_joboko.json` | Joboko | ~7,522 records | Viá»‡c lÃ m Ä‘a dáº¡ng táº¡i Viá»‡t Nam |
| `data_topcv.json` | TopCV | ~2,420 records | Viá»‡c lÃ m IT/CNTT táº¡i Viá»‡t Nam |

### Cáº¥u trÃºc Dá»¯ liá»‡u
- **CareerLink**: TÃªn cÃ´ng viá»‡c, TÃªn cÃ´ng ty, Äá»‹a Ä‘iá»ƒm, Má»©c lÆ°Æ¡ng, Kinh nghiá»‡m, MÃ´ táº£, Ká»¹ nÄƒng yÃªu cáº§u
- **Joboko**: TÃªn cÃ´ng viá»‡c, TÃªn cÃ´ng ty, Äá»‹a Ä‘iá»ƒm, Má»©c lÆ°Æ¡ng, Kinh nghiá»‡m, MÃ´ táº£, Ká»¹ nÄƒng yÃªu cáº§u, NgÃ nh nghá»
- **TopCV**: TÃªn cÃ´ng viá»‡c, TÃªn cÃ´ng ty, Äá»‹a Ä‘iá»ƒm, Má»©c lÆ°Æ¡ng, Kinh nghiá»‡m, MÃ´ táº£, Ká»¹ nÄƒng yÃªu cáº§u, Quyá»n lá»£i

## ðŸ—ï¸ Kiáº¿n trÃºc Há»‡ thá»‘ng

```
ðŸ“¥ DATA SOURCES
â”œâ”€â”€ CareerLink (Vietnam IT-focused)
â”œâ”€â”€ Joboko (Vietnam diverse)
â””â”€â”€ TopCV (Vietnam IT/CNTT)

ðŸ”„ DATA INGESTION LAYER
â”œâ”€â”€ Batch Processing: Apache Spark/PySpark
â”œâ”€â”€ Data Cleaning & Standardization
â””â”€â”€ Schema Mapping & Transformation

ðŸ’¾ STORAGE LAYER
â”œâ”€â”€ Raw Data: HDFS/S3
â”œâ”€â”€ Processed Data: Hive/Delta Lake
â””â”€â”€ Analytics Ready: PostgreSQL/MongoDB

âš¡ PROCESSING LAYER
â”œâ”€â”€ Batch Analytics: Spark SQL, Pandas
â”œâ”€â”€ Real-time: Kafka + Spark Streaming
â””â”€â”€ ML Pipeline: Scikit-learn, TensorFlow

ðŸ“Š ANALYTICS LAYER
â”œâ”€â”€ Dashboard: Streamlit/Dash
â”œâ”€â”€ API: FastAPI/Flask
â””â”€â”€ Reports: Jupyter Notebooks
```

## ðŸ› ï¸ CÃ´ng nghá»‡ Sá»­ dá»¥ng

### Core Technologies
- **Python 3.8+**: NgÃ´n ngá»¯ chÃ­nh
- **Apache Spark**: Xá»­ lÃ½ dá»¯ liá»‡u lá»›n
- **Pandas**: Data manipulation
- **NumPy**: Numerical computing
- **Scikit-learn**: Machine learning

### Database & Storage
- **PostgreSQL**: Structured data storage
- **MongoDB**: Semi-structured data
- **HDFS**: Distributed file system
- **Delta Lake**: Data lake storage

### Analytics & ML
- **NLTK/spaCy**: Natural language processing
- **TensorFlow/PyTorch**: Deep learning
- **Prophet**: Time series forecasting
- **Plotly/Matplotlib**: Data visualization

### Infrastructure
- **Docker**: Containerization
- **Jupyter**: Interactive analysis
- **Streamlit**: Web dashboard
- **FastAPI**: REST API

## ðŸ“‹ Káº¿ hoáº¡ch Triá»ƒn khai

### Phase 1: Data Preparation (2-3 tuáº§n)
- [ ] **Data Cleaning & Standardization**
  - Xá»­ lÃ½ missing values, duplicates
  - Standardize job titles, locations, skills
  - Create unified schema

- [ ] **Data Integration**
  - Map common fields across datasets
  - Create master data dictionary
  - Implement data quality checks

### Phase 2: Infrastructure Setup (1-2 tuáº§n)
- [ ] **Environment Setup**
  - Docker containers for reproducibility
  - Jupyter notebooks for analysis
  - Database setup (PostgreSQL/MongoDB)

- [ ] **Data Pipeline**
  - ETL scripts (Python/PySpark)
  - Data validation framework
  - Monitoring and logging

### Phase 3: Analytics Implementation (3-4 tuáº§n)
- [ ] **Exploratory Data Analysis**
  - Statistical summaries
  - Data visualization
  - Correlation analysis

- [ ] **Machine Learning Models**
  - Salary prediction model
  - Skills clustering
  - Sentiment analysis

### Phase 4: Visualization & Reporting (1-2 tuáº§n)
- [ ] **Dashboard Development**
  - Interactive dashboards
  - Real-time analytics
  - Export capabilities

- [ ] **Documentation & Presentation**
  - Technical documentation
  - Business insights report
  - Demo preparation

## ðŸŽ¯ CÃ¡c BÃ i toÃ¡n PhÃ¢n tÃ­ch

### 1. **PhÃ¢n tÃ­ch Xu hÆ°á»›ng Thá»‹ trÆ°á»ng Viá»‡c lÃ m**
- **Má»¥c tiÃªu**: PhÃ¢n tÃ­ch xu hÆ°á»›ng viá»‡c lÃ m theo thá»i gian, Ä‘á»‹a lÃ½, ngÃ nh nghá»
- **PhÆ°Æ¡ng phÃ¡p**: Time series analysis, Geographic analysis
- **Káº¿t quáº£**: Dashboard hiá»ƒn thá»‹ hot jobs, declining jobs, regional trends

### 2. **PhÃ¢n tÃ­ch Má»©c lÆ°Æ¡ng vÃ  Yáº¿u tá»‘ áº¢nh hÆ°á»Ÿng**
- **Má»¥c tiÃªu**: Dá»± Ä‘oÃ¡n má»©c lÆ°Æ¡ng dá»±a trÃªn skills, experience, location
- **PhÆ°Æ¡ng phÃ¡p**: Regression analysis, Feature engineering
- **Káº¿t quáº£**: Salary prediction model, compensation insights

### 3. **PhÃ¢n tÃ­ch Ká»¹ nÄƒng vÃ  YÃªu cáº§u**
- **Má»¥c tiÃªu**: XÃ¡c Ä‘á»‹nh skills quan trá»ng nháº¥t cho tá»«ng vá»‹ trÃ­
- **PhÆ°Æ¡ng phÃ¡p**: NLP, Text mining, Clustering
- **Káº¿t quáº£**: Skills taxonomy, skill gap analysis

### 4. **PhÃ¢n tÃ­ch Cáº£m xÃºc vÃ  MÃ´ táº£ CÃ´ng viá»‡c**
- **Má»¥c tiÃªu**: PhÃ¢n tÃ­ch sentiment trong job descriptions
- **PhÆ°Æ¡ng phÃ¡p**: NLP, Sentiment analysis
- **Káº¿t quáº£**: Company culture insights, job attractiveness score

### 5. **PhÃ¢n tÃ­ch Cáº¡nh tranh vÃ  Thá»‹ trÆ°á»ng**
- **Má»¥c tiÃªu**: So sÃ¡nh thá»‹ trÆ°á»ng viá»‡c lÃ m giá»¯a cÃ¡c quá»‘c gia
- **PhÆ°Æ¡ng phÃ¡p**: Comparative analysis, Statistical modeling
- **Káº¿t quáº£**: Market comparison dashboard

## ðŸš€ CÃ i Ä‘áº·t vÃ  Cháº¡y Dá»± Ã¡n

### YÃªu cáº§u Há»‡ thá»‘ng
- Python 3.8+
- Docker & Docker Compose
- PostgreSQL 12+
- Apache Spark 3.0+


1) Khá»Ÿi Ä‘á»™ng cÆ¡ sá»Ÿ dá»¯ liá»‡u báº±ng Docker

```bash
docker-compose up -d postgres mongodb

# Kiá»ƒm tra container
docker ps --filter "name=job_analytics_"
```

- PostgreSQL sáº½ cháº¡y táº¡i `localhost:5432` vá»›i DB `job_analytics`, user `admin`, password `password123`.
- MongoDB sáº½ cháº¡y táº¡i `localhost:27017` vá»›i user `admin`, password `password123`.

2) XÃ¡c nháº­n PostgreSQL Ä‘Ã£ khá»Ÿi táº¡o báº£ng (tÃ¹y chá»n)

```bash
# CÃ i psql náº¿u cáº§n, rá»“i cháº¡y:
PGPASSWORD=password123 psql -h 127.0.0.1 -p 5432 -U admin -d job_analytics -c "\dt"
```

Báº¡n sáº½ tháº¥y cÃ¡c báº£ng nhÆ° `processed_jobs`, `salary_analysis`, `skills_analysis`, `market_trends` Ä‘Æ°á»£c táº¡o tá»« `config/init.sql`.

3) LÆ°u Ã½ xÃ¡c thá»±c MongoDB (trÃ¡nh lá»—i auth)

- NgÆ°á»i dÃ¹ng `admin` máº·c Ä‘á»‹nh náº±m á»Ÿ database `admin`. Náº¿u báº¡n gáº·p lá»—i xÃ¡c thá»±c khi cháº¡y pipeline, dÃ¹ng chuá»—i káº¿t ná»‘i cÃ³ `authSource=admin`.
- VÃ­ dá»¥ chuá»—i káº¿t ná»‘i an toÃ n:

```text
mongodb://admin:password123@localhost:27017/job_analytics?authSource=admin
```

Náº¿u cáº§n, cÃ³ thá»ƒ sá»­a trong file `database_analytics_pipeline.py` (biáº¿n `mongodb_connection_string`).

4) Táº¡o mÃ´i trÆ°á»ng Python vÃ  cÃ i dependency

```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux

pip install --upgrade pip
pip install -r requirements.txt
```

5) Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘áº§u vÃ o

- Äáº£m báº£o cÃ¡c file trong thÆ° má»¥c `data/` tá»“n táº¡i: `data_careerlink.json`, `data_joboko.json`, `data_topcv.json`.
- Module `src/etl/data_loader.py` sáº½ Ä‘á»c cÃ¡c nguá»“n nÃ y khi pipeline cháº¡y.

6) Cháº¡y pipeline

```bash
python database_analytics_pipeline.py
```

Pipeline sáº½:
- Náº¡p vÃ  lÃ m sáº¡ch dá»¯ liá»‡u (ETL) báº±ng `DataLoader` vÃ  `DataCleaner`
- LÆ°u dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a vÃ o PostgreSQL báº£ng `processed_jobs`
- LÆ°u dá»¯ liá»‡u thÃ´/Ä‘Ã£ xá»­ lÃ½ vÃ o MongoDB (náº¿u báº­t)
- Táº¡o bÃ¡o cÃ¡o phÃ¢n tÃ­ch toÃ n diá»‡n vÃ  lÆ°u káº¿t quáº£ vÃ o cÃ¡c báº£ng analytics

7) Kiá»ƒm tra nhanh dá»¯ liá»‡u sau khi cháº¡y (tÃ¹y chá»n)

```bash
# Äáº¿m báº£n ghi processed_jobs
PGPASSWORD=password123 psql -h 127.0.0.1 -p 5432 -U admin -d job_analytics -c "SELECT COUNT(*) FROM processed_jobs;"
```

8) Xá»­ lÃ½ sá»± cá»‘ thÆ°á»ng gáº·p

- Cá»•ng 5432/27017 Ä‘Ã£ báº­n: dá»«ng dá»‹ch vá»¥ khÃ¡c hoáº·c Ä‘á»•i cá»•ng Ã¡nh xáº¡ trong `docker-compose.yml`.
- Lá»—i MongoDB Authentication: dÃ¹ng chuá»—i káº¿t ná»‘i cÃ³ `?authSource=admin` nhÆ° hÆ°á»›ng dáº«n á»Ÿ bÆ°á»›c 3.
- Thiáº¿u thÆ° viá»‡n Python: Ä‘áº£m báº£o Ä‘Ã£ cÃ i `requirements.txt` trong Ä‘Ãºng virtualenv.
- Lá»—i káº¿t ná»‘i DB: kiá»ƒm tra container Ä‘ang cháº¡y vÃ  network ná»™i bá»™ Docker hoáº¡t Ä‘á»™ng (`docker ps`).

## ðŸ“ Cáº¥u trÃºc ThÆ° má»¥c

```
tichhop-git/
â”œâ”€â”€ data/                    # Raw datasets
â”‚   â”œâ”€â”€ data_careerlink.json
â”‚   â”œâ”€â”€ data_joboko.json
â”‚   â””â”€â”€ data_topcv.json
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ etl/                # ETL pipelines
â”‚   â”œâ”€â”€ analytics/          # Analytics modules
â”‚   â”œâ”€â”€ models/             # ML models
â”‚   â””â”€â”€ utils/              # Utility functions
â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”œâ”€â”€ dashboard/              # Streamlit dashboard
â”œâ”€â”€ api/                    # FastAPI endpoints
â”œâ”€â”€ config/                 # Configuration files
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ docker/                 # Docker configurations
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ docker-compose.yml      # Docker compose
â””â”€â”€ README.md              # This file
```

## ðŸ“Š Káº¿t quáº£ Mong Ä‘á»£i

### Dashboard Features
- **Job Market Trends**: Biá»ƒu Ä‘á»“ xu hÆ°á»›ng viá»‡c lÃ m theo thá»i gian
- **Salary Analysis**: PhÃ¢n tÃ­ch má»©c lÆ°Æ¡ng theo ngÃ nh, Ä‘á»‹a Ä‘iá»ƒm
- **Skills Demand**: Top skills Ä‘Æ°á»£c yÃªu cáº§u nhiá»u nháº¥t
- **Geographic Insights**: So sÃ¡nh thá»‹ trÆ°á»ng viá»‡c lÃ m theo vÃ¹ng
- **Company Analysis**: PhÃ¢n tÃ­ch cÃ´ng ty vÃ  vÄƒn hÃ³a lÃ m viá»‡c

### API Endpoints
- `GET /api/jobs` - Láº¥y danh sÃ¡ch viá»‡c lÃ m
- `GET /api/salary/predict` - Dá»± Ä‘oÃ¡n má»©c lÆ°Æ¡ng
- `GET /api/trends` - Xu hÆ°á»›ng thá»‹ trÆ°á»ng
- `GET /api/skills/analysis` - PhÃ¢n tÃ­ch ká»¹ nÄƒng

## ðŸ”§ Development

### Code Style
- PEP 8 compliance
- Type hints
- Docstrings
- Unit tests

### Git Workflow
```bash
# Táº¡o feature branch
git checkout -b feature/new-analysis

# Commit changes
git add .
git commit -m "Add new analysis module"

# Push vÃ  táº¡o PR
git push origin feature/new-analysis
```

## ðŸ“ˆ Metrics & KPIs

- **Data Quality**: >95% data completeness
- **Processing Speed**: <5 minutes for full pipeline
- **Model Accuracy**: >85% for salary prediction
- **Dashboard Load Time**: <3 seconds

## ðŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Create Pull Request

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸ“ž Contact

- **Author**: [Nguyá»…n Thuá»³ DÆ°Æ¡ng]
- **Email**: [Duong.NT252022M@sis.hust.edu.vn]

---

## ðŸ“š References

- [Apache Spark Documentation](https://spark.apache.org/docs/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

*Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho mÃ´n TÃ­ch há»£p Dá»¯ liá»‡u Lá»›n - Final Project*
