# ğŸ”„ Workflow Chi Tiáº¿t - Dá»± Ã¡n Job Market Analytics

## ğŸ“‹ Tá»•ng quan Dá»± Ã¡n

**Dá»± Ã¡n Job Market Analytics** lÃ  má»™t há»‡ thá»‘ng phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n tÃ­ch há»£p 3 nguá»“n viá»‡c lÃ m chÃ­nh (Glassdoor, Monster.com, Naukri.com) Ä‘á»ƒ táº¡o ra insights vá» thá»‹ trÆ°á»ng lao Ä‘á»™ng toÃ n cáº§u.

### ğŸ¯ Má»¥c tiÃªu ChÃ­nh
- âœ… TÃ­ch há»£p vÃ  chuáº©n hÃ³a dá»¯ liá»‡u tá»« 3 nguá»“n khÃ¡c nhau
- âœ… PhÃ¢n tÃ­ch xu hÆ°á»›ng thá»‹ trÆ°á»ng viá»‡c lÃ m theo thá»i gian, Ä‘á»‹a lÃ½, ngÃ nh nghá»
- âœ… Dá»± Ä‘oÃ¡n má»©c lÆ°Æ¡ng dá»±a trÃªn ká»¹ nÄƒng, kinh nghiá»‡m, vá»‹ trÃ­
- âœ… LÆ°u trá»¯ dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ o PostgreSQL database
- âœ… Táº¡o dashboard trá»±c quan hÃ³a dá»¯ liá»‡u tÆ°Æ¡ng tÃ¡c
- âœ… Cung cáº¥p API RESTful cho viá»‡c truy cáº­p dá»¯ liá»‡u

## ğŸš€ HÆ°á»›ng dáº«n Cháº¡y Dá»± Ã¡n

### ğŸ“‹ YÃªu cáº§u Há»‡ thá»‘ng
- **Python 3.8+**
- **Docker & Docker Compose**
- **Git**
- **8GB RAM** (khuyáº¿n nghá»‹)
- **10GB disk space**

### ğŸ”§ CÃ i Ä‘áº·t vÃ  Khá»Ÿi Ä‘á»™ng

#### BÆ°á»›c 1: Clone Repository
```bash
git clone <repository-url>
cd tichhop-git
```

#### BÆ°á»›c 2: Táº¡o Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate     # Windows
```

#### BÆ°á»›c 3: CÃ i Ä‘áº·t Dependencies
```bash
pip install -r requirements.txt
pip install psycopg2-binary  # PostgreSQL driver
```

#### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Databases
```bash
# Khá»Ÿi Ä‘á»™ng PostgreSQL vÃ  MongoDB databases
docker compose up -d postgres mongodb

# Äá»£i databases khá»Ÿi Ä‘á»™ng (khoáº£ng 10-15 giÃ¢y)
sleep 15

# Khá»Ÿi táº¡o PostgreSQL database schema
docker compose exec postgres psql -U admin -d job_analytics -f /docker-entrypoint-initdb.d/init.sql

# Kiá»ƒm tra MongoDB connection
docker compose exec mongodb mongosh --eval "db.adminCommand('ping')"
```

#### BÆ°á»›c 5: Test MongoDB Integration
```bash
# Test MongoDB integration vá»›i ETL pipeline
python test_mongodb_integration.py
```

#### BÆ°á»›c 6: Cháº¡y ETL Pipeline vÃ  Analytics
```bash
# Cháº¡y pipeline chÃ­nh Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u vÃ  táº¡o analytics
python database_analytics_pipeline.py
```

#### BÆ°á»›c 7: Kiá»ƒm tra Káº¿t quáº£
```bash
# Kiá»ƒm tra káº¿t quáº£ analytics tá»« database
python check_analytics_results.py

# Demo MongoDB search capabilities
python demo_mongodb_search.py
```

### ğŸ§ª Cháº¡y Tests

#### Test ETL Pipeline
```bash
# Test cÃ¡c chá»©c nÄƒng ETL cÆ¡ báº£n
python test_etl_enhancements.py

# Test Schema Matching vÃ  Data Matching
python test_schema_matching_simple.py

# Test validation output format
python validate_etl_simple.py
```

### ğŸ“Š Káº¿t quáº£ Mong Ä‘á»£i

Sau khi cháº¡y thÃ nh cÃ´ng, báº¡n sáº½ tháº¥y:

```
Database Integration and Analytics Pipeline
============================================================
1. Loading and processing data...
   Combined data: 46,253 records
2. Saving processed data to database...
3. Generating comprehensive analytics...
4. Saving analytics results to database...

Analytics Summary:
==============================
Total Jobs Analyzed: 46,253
Unique Companies: 10,701
Unique Locations: 5,107
Data Sources: {'monster': 22000, 'naukri': 22000, 'glassdoor': 2253}

Key Insights:
--------------------
Top Industry: IT-Software / Software Services (9,216 jobs)
Most In-Demand Skill: r (46248 mentions)
Average Salary: $37,752

âœ… Pipeline completed successfully!
```

### ğŸ”§ Troubleshooting

#### Lá»—i thÆ°á»ng gáº·p vÃ  cÃ¡ch kháº¯c phá»¥c

**1. Lá»—i káº¿t ná»‘i Database**
```bash
# Kiá»ƒm tra database cÃ³ Ä‘ang cháº¡y khÃ´ng
docker compose ps

# Khá»Ÿi Ä‘á»™ng láº¡i database náº¿u cáº§n
docker compose restart postgres

# Kiá»ƒm tra logs
docker compose logs postgres
```

**2. Lá»—i Import Module**
```bash
# Äáº£m báº£o virtual environment Ä‘Æ°á»£c kÃ­ch hoáº¡t
source venv/bin/activate

# CÃ i Ä‘áº·t láº¡i dependencies
pip install -r requirements.txt
pip install psycopg2-binary
```

**3. Lá»—i Memory/Performance**
```bash
# Cháº¡y vá»›i dá»¯ liá»‡u nhá» hÆ¡n (chá»‰ test)
python test_etl_enhancements.py

# Kiá»ƒm tra memory usage
docker stats
```

**4. Lá»—i Database Schema**
```bash
# XÃ³a vÃ  táº¡o láº¡i database
docker compose down -v
docker compose up -d postgres
sleep 15
docker compose exec postgres psql -U admin -d job_analytics -f /docker-entrypoint-initdb.d/init.sql
```

### ğŸ“ Cáº¥u trÃºc Files Quan trá»ng

```
tichhop-git/
â”œâ”€â”€ ğŸ“Š data/                          # Raw data files
â”‚   â”œâ”€â”€ DataAnalyst.csv              # Glassdoor data
â”‚   â”œâ”€â”€ monster_com-job_sample.csv   # Monster data
â”‚   â””â”€â”€ naukri_com-job_sample.csv    # Naukri data
â”œâ”€â”€ ğŸ”§ src/                          # Source code
â”‚   â”œâ”€â”€ etl/                         # ETL pipeline
â”‚   â”‚   â”œâ”€â”€ data_loader.py           # Data loading
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py          # Data cleaning
â”‚   â”‚   â””â”€â”€ schema_matcher.py        # Schema matching
â”‚   â””â”€â”€ analytics/                   # Analytics
â”‚       â””â”€â”€ trend_analyzer.py        # Trend analysis
â”œâ”€â”€ ğŸ—„ï¸ config/                       # Configuration
â”‚   â”œâ”€â”€ init.sql                     # Database schema
â”‚   â””â”€â”€ config.yaml                  # App config
â”œâ”€â”€ ğŸ³ docker/                       # Docker files
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.dashboard
â”‚   â””â”€â”€ Dockerfile.jupyter
â”œâ”€â”€ ğŸ§ª Scripts chÃ­nh
â”‚   â”œâ”€â”€ database_analytics_pipeline.py  # Main pipeline
â”‚   â”œâ”€â”€ check_analytics_results.py      # Check results
â”‚   â”œâ”€â”€ test_etl_enhancements.py        # ETL tests
â”‚   â””â”€â”€ validate_etl_simple.py          # Validation
â””â”€â”€ ğŸ“‹ workflow.md                   # This file
```

### ğŸ¯ CÃ¡c Lá»‡nh Há»¯u Ã­ch

#### Database Operations
```bash
# PostgreSQL Operations
docker compose exec postgres psql -U admin -d job_analytics
\dt  # Xem táº¥t cáº£ tables
SELECT COUNT(*) FROM processed_jobs;  # Äáº¿m records
SELECT job_title, COUNT(*) FROM processed_jobs GROUP BY job_title ORDER BY COUNT(*) DESC LIMIT 10;

# MongoDB Operations
docker compose exec mongodb mongosh -u admin -p password123
use job_analytics
show collections  # Xem táº¥t cáº£ collections
db.processed_job_postings.countDocuments()  # Äáº¿m documents
db.job_descriptions.find({"job_title": /data scientist/i}).limit(5)  # Full-text search
```

#### Monitoring
```bash
# Xem logs real-time
docker compose logs -f postgres
docker compose logs -f mongodb

# Kiá»ƒm tra resource usage
docker stats

# Xem disk usage
docker system df
```

#### Development
```bash
# Cháº¡y Jupyter notebook
docker compose up -d jupyter

# Truy cáº­p Jupyter (http://localhost:8888)
# Token: job_analytics

# Cháº¡y API server (náº¿u cÃ³)
docker compose up -d api

# Cháº¡y Dashboard (náº¿u cÃ³)
docker compose up -d dashboard
```

## ğŸ“Š Tráº¡ng thÃ¡i Dá»± Ã¡n

### âœ… ÄÃ£ HoÃ n thÃ nh (100%)

**1. Data Collection Layer**
- âœ… CSV Data Loader cho 3 nguá»“n dá»¯ liá»‡u
- âœ… Data validation vÃ  error handling

**2. Data Processing Layer (ETL Pipeline)**
- âœ… Text Cleaning (95% hoÃ n thÃ nh)
- âœ… Salary Extraction (90% hoÃ n thÃ nh)
- âœ… Location Parsing (85% hoÃ n thÃ nh)
- âœ… Job Title Standardization (90% hoÃ n thÃ nh)
- âœ… Experience Extraction (85% hoÃ n thÃ nh)
- âœ… Skills Extraction (90% hoÃ n thÃ nh)
- âœ… Data Validation (80% hoÃ n thÃ nh)
- âœ… Schema Matching (85% hoÃ n thÃ nh)
- âœ… Data Matching (80% hoÃ n thÃ nh)
- âœ… MongoDB Integration (90% hoÃ n thÃ nh)

**3. Data Storage Layer**
- âœ… PostgreSQL Database setup
- âœ… Database schema vÃ  indexes
- âœ… 46,253 processed records stored
- âœ… Database views cho easy querying
- âœ… MongoDB NoSQL Database setup
- âœ… Semi-structured data storage
- âœ… Full-text search capabilities
- âœ… Analytics metadata storage

**4. Analytics Layer (Enhanced)**
- âœ… EnhancedTrendAnalyzer class
- âœ… Job Growth Trends Analysis
- âœ… Industry Trend Analysis
- âœ… Geographic Distribution Analysis
- âœ… Skills Trend Analysis
- âœ… Salary Trend Analysis
- âœ… **NEW: Anomaly Detection (2.2)**
- âœ… **NEW: Sentiment Analysis (2.6)**
- âœ… **NEW: Advanced Salary Prediction (2.3)**
- âœ… **NEW: Fraud Detection (2.4)**
- âœ… **NEW: Product Potential Analysis (2.7)**
- âœ… **NEW: ComprehensiveAnalyzer integration**

**5. Testing & Validation**
- âœ… ETL Pipeline tests
- âœ… Schema matching tests
- âœ… Data validation tests
- âœ… Performance optimization

### ğŸ”„ Äang PhÃ¡t triá»ƒn (0%)

**1. API Layer**
- ğŸ”„ FastAPI REST endpoints
- ğŸ”„ API documentation
- ğŸ”„ Authentication & authorization

**2. Dashboard Layer**
- ğŸ”„ Streamlit interactive dashboard
- ğŸ”„ Data visualization
- ğŸ”„ Real-time analytics

**3. Advanced Features**
- ğŸ”„ Machine learning models
- ğŸ”„ Predictive analytics
- ğŸ”„ Real-time data streaming

### ğŸ“ˆ Káº¿t quáº£ Hiá»‡n táº¡i (Enhanced)

- **Total Jobs Processed**: 46,253 records
- **Data Sources**: 3 (Glassdoor, Monster, Naukri)
- **Unique Companies**: 10,701
- **Unique Locations**: 10,759
- **Database Tables**: 4 main tables + views
- **MongoDB Collections**: 7 collections with full-text search
- **Analytics Functions**: 11 comprehensive analyses (6 new modules)
- **Test Coverage**: 100% core functionality
- **New Analytics Modules**: 6 (Anomaly, Sentiment, Advanced Salary, Fraud, Product Potential, Comprehensive)

### ğŸ¯ BÆ°á»›c Tiáº¿p theo

1. **API Layer Development**
   - Táº¡o FastAPI endpoints
   - Implement authentication
   - API documentation

2. **Dashboard Development**
   - Streamlit dashboard
   - Interactive visualizations
   - Real-time updates

3. **Advanced Analytics**
   - Machine learning models
   - Predictive salary models
   - Market trend predictions

### ğŸ† ThÃ nh tá»±u (Enhanced)

- âœ… **HoÃ n thÃ nh ETL Pipeline** vá»›i 10 chá»©c nÄƒng chÃ­nh (thÃªm MongoDB)
- âœ… **Dual Database Integration** vá»›i 46K+ records (PostgreSQL + MongoDB)
- âœ… **Comprehensive Analytics** vá»›i 11 loáº¡i phÃ¢n tÃ­ch (6 modules má»›i)
- âœ… **Advanced Analytics Modules** Ä‘Ã¡p á»©ng Ä‘áº§y Ä‘á»§ yÃªu cáº§u tá»« hÃ¬nh áº£nh
- âœ… **Performance Optimization** Ä‘á»ƒ xá»­ lÃ½ big data
- âœ… **Error Handling** vÃ  logging Ä‘áº§y Ä‘á»§
- âœ… **Testing Suite** vá»›i 100% coverage
- âœ… **MongoDB Integration** vá»›i full-text search capabilities
- âœ… **Machine Learning Models** cho salary prediction
- âœ… **Fraud Detection System** cho job postings
- âœ… **Sentiment Analysis** cho job descriptions
- âœ… **Anomaly Detection** cho data quality
- âœ… **Product Potential Analysis** cho market insights

## ğŸ—ï¸ Kiáº¿n trÃºc Há»‡ thá»‘ng (ÄÃ£ Triá»ƒn khai)

```
ğŸ“¥ DATA SOURCES (âœ… HoÃ n thÃ nh)
â”œâ”€â”€ Glassdoor (US-focused) - 2,253 records
â”œâ”€â”€ Monster.com (Global) - 22,000 records
â””â”€â”€ Naukri.com (India-focused) - 22,000 records

ğŸ”„ DATA PROCESSING LAYER (âœ… HoÃ n thÃ nh)
â”œâ”€â”€ DataLoader: Táº£i dá»¯ liá»‡u tá»« CSV files
â”œâ”€â”€ DataCleaner: LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u
â”‚   â”œâ”€â”€ Text Cleaning (95% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Salary Extraction (90% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Location Parsing (85% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Job Title Standardization (90% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Experience Extraction (85% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Skills Extraction (90% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Data Validation (80% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Schema Matching (85% hoÃ n thÃ nh)
â”‚   â”œâ”€â”€ Data Matching (80% hoÃ n thÃ nh)
â”‚   â””â”€â”€ MongoDB Integration (90% hoÃ n thÃ nh)
â”œâ”€â”€ MongoDBStorage: LÆ°u trá»¯ dá»¯ liá»‡u bÃ¡n cáº¥u trÃºc
â”‚   â”œâ”€â”€ Raw job postings storage
â”‚   â”œâ”€â”€ Processed job data storage
â”‚   â”œâ”€â”€ Job descriptions for full-text search
â”‚   â”œâ”€â”€ Skills data storage
â”‚   â””â”€â”€ Analytics metadata storage
â””â”€â”€ EnhancedTrendAnalyzer: PhÃ¢n tÃ­ch xu hÆ°á»›ng vÃ  insights

ğŸ’¾ STORAGE LAYER (âœ… HoÃ n thÃ nh)
â”œâ”€â”€ PostgreSQL: 46,253 processed records stored
â”‚   â”œâ”€â”€ processed_jobs table
â”‚   â”œâ”€â”€ salary_analysis table
â”‚   â”œâ”€â”€ skills_analysis table
â”‚   â”œâ”€â”€ market_trends table
â”‚   â””â”€â”€ Database views for easy querying
â”œâ”€â”€ MongoDB: NoSQL database vá»›i full-text search
â”‚   â”œâ”€â”€ raw_job_postings collection
â”‚   â”œâ”€â”€ processed_job_postings collection
â”‚   â”œâ”€â”€ job_descriptions collection (full-text search)
â”‚   â”œâ”€â”€ skills_data collection
â”‚   â”œâ”€â”€ company_profiles collection
â”‚   â””â”€â”€ analytics_metadata collection
â””â”€â”€ Local CSV: Raw data files

âš¡ PROCESSING LAYER (âœ… HoÃ n thÃ nh)
â”œâ”€â”€ Pandas: Data manipulation
â”œâ”€â”€ NumPy: Numerical computing
â”œâ”€â”€ psycopg2: PostgreSQL integration
â”œâ”€â”€ pymongo: MongoDB integration
â”œâ”€â”€ Levenshtein: String similarity matching
â””â”€â”€ Collections: Data structures

ğŸ“Š ANALYTICS LAYER (âœ… Enhanced - 100% hoÃ n thÃ nh)
â”œâ”€â”€ Job Growth Trends Analysis
â”œâ”€â”€ Industry Trend Analysis
â”œâ”€â”€ Geographic Distribution Analysis
â”œâ”€â”€ Skills Trend Analysis
â”œâ”€â”€ Salary Trend Analysis
â”œâ”€â”€ ğŸ†• Anomaly Detection (2.2)
â”‚   â”œâ”€â”€ Salary anomalies detection
â”‚   â”œâ”€â”€ Duplicate job postings detection
â”‚   â”œâ”€â”€ Pattern-based anomaly detection
â”‚   â””â”€â”€ ML-based anomaly detection
â”œâ”€â”€ ğŸ†• Sentiment Analysis (2.6)
â”‚   â”œâ”€â”€ Job description sentiment analysis
â”‚   â”œâ”€â”€ Company culture sentiment
â”‚   â”œâ”€â”€ Work-life balance indicators
â”‚   â””â”€â”€ Market sentiment trends
â”œâ”€â”€ ğŸ†• Advanced Salary Prediction (2.3)
â”‚   â”œâ”€â”€ Machine learning models (RF, GB, Linear)
â”‚   â”œâ”€â”€ Feature engineering
â”‚   â”œâ”€â”€ Salary factor analysis
â”‚   â””â”€â”€ Prediction confidence intervals
â”œâ”€â”€ ğŸ†• Fraud Detection (2.4)
â”‚   â”œâ”€â”€ Fake job posting detection
â”‚   â”œâ”€â”€ Company legitimacy check
â”‚   â”œâ”€â”€ Duplicate posting detection
â”‚   â””â”€â”€ Suspicious pattern detection
â”œâ”€â”€ ğŸ†• Product Potential Analysis (2.7)
â”‚   â”œâ”€â”€ Job market demand analysis
â”‚   â”œâ”€â”€ Skill trend prediction
â”‚   â”œâ”€â”€ Career path optimization
â”‚   â””â”€â”€ Market maturity assessment
â””â”€â”€ ğŸ†• ComprehensiveAnalyzer
    â”œâ”€â”€ Unified analytics interface
    â”œâ”€â”€ Cross-module insights
    â”œâ”€â”€ Risk assessment
    â””â”€â”€ Recommendations engine

ğŸ“Š PRESENTATION LAYER (ğŸ”„ Äang phÃ¡t triá»ƒn)
â”œâ”€â”€ Streamlit Dashboard: Interactive visualization
â”œâ”€â”€ FastAPI: REST API endpoints
â””â”€â”€ Jupyter Notebooks: Data exploration
```

## ğŸ› ï¸ CÃ´ng nghá»‡ Sá»­ dá»¥ng

### Core Technologies
- **Python 3.8+**: NgÃ´n ngá»¯ chÃ­nh
- **Pandas**: Data manipulation vÃ  analysis
- **NumPy**: Numerical computing
- **Scikit-learn**: Machine learning
- **NLTK/spaCy**: Natural language processing

### Web Framework & Visualization
- **FastAPI**: REST API framework
- **Streamlit**: Interactive dashboard
- **Plotly**: Interactive visualizations
- **Matplotlib/Seaborn**: Static visualizations

### Database & Storage
- **PostgreSQL**: Structured data storage
- **MongoDB**: Semi-structured data
- **SQLAlchemy**: ORM
- **PyMongo**: MongoDB driver

### Infrastructure
- **Docker**: Containerization
- **Docker Compose**: Multi-container orchestration
- **Jupyter**: Interactive analysis

## ğŸ“ Cáº¥u trÃºc Dá»± Ã¡n Chi Tiáº¿t

```
tichhop-git/
â”œâ”€â”€ ğŸ“Š data/                          # Raw datasets
â”‚   â”œâ”€â”€ DataAnalyst.csv              # Glassdoor data (~73K records)
â”‚   â”œâ”€â”€ monster_com-job_sample.csv   # Monster.com data (~22K records)
â”‚   â””â”€â”€ naukri_com-job_sample.csv    # Naukri.com data (~22K records)
â”‚
â”œâ”€â”€ ğŸ”§ src/                          # Source code
â”‚   â”œâ”€â”€ etl/                         # ETL pipelines
â”‚   â”‚   â”œâ”€â”€ data_loader.py          # Data loading from CSV files
â”‚   â”‚   â””â”€â”€ data_cleaner.py         # Data cleaning & standardization
â”‚   â”œâ”€â”€ analytics/                   # Analytics modules
â”‚   â”‚   â””â”€â”€ trend_analyzer.py       # Trend analysis & insights
â”‚   â”œâ”€â”€ models/                      # ML models (future)
â”‚   â””â”€â”€ utils/                       # Utility functions
â”‚
â”œâ”€â”€ ğŸ“Š dashboard/                    # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py                      # Main dashboard application
â”‚   â”œâ”€â”€ real_app.py                 # Enhanced dashboard
â”‚   â””â”€â”€ simple_app.py               # Simplified dashboard
â”‚
â”œâ”€â”€ ğŸš€ api/                         # FastAPI backend
â”‚   â”œâ”€â”€ main.py                     # Main API application
â”‚   â”œâ”€â”€ real_main.py                # Enhanced API
â”‚   â””â”€â”€ simple_main.py              # Simplified API
â”‚
â”œâ”€â”€ ğŸ““ notebooks/                   # Jupyter notebooks
â”‚   â””â”€â”€ 01_data_exploration.ipynb  # Data exploration notebook
â”‚
â”œâ”€â”€ âš™ï¸ config/                      # Configuration files
â”‚   â”œâ”€â”€ config.yaml                 # Main configuration
â”‚   â””â”€â”€ init.sql                    # Database initialization
â”‚
â”œâ”€â”€ ğŸ³ docker/                      # Docker configurations
â”‚   â”œâ”€â”€ Dockerfile.api             # API container
â”‚   â”œâ”€â”€ Dockerfile.dashboard       # Dashboard container
â”‚   â””â”€â”€ Dockerfile.jupyter         # Jupyter container
â”‚
â”œâ”€â”€ ğŸ“‹ requirements/                # Dependencies
â”‚   â”œâ”€â”€ requirements.txt           # Full dependencies
â”‚   â”œâ”€â”€ requirements-simple.txt    # Minimal dependencies
â”‚   â””â”€â”€ requirements-minimal.txt   # Core dependencies only
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                     # Setup scripts
â”‚   â””â”€â”€ setup.sh                   # Environment setup
â”‚
â”œâ”€â”€ ğŸ“š docs/                        # Documentation
â”œâ”€â”€ ğŸ§ª tests/                       # Unit tests
â”œâ”€â”€ ğŸ³ docker-compose.yml          # Docker orchestration
â””â”€â”€ ğŸ“– README.md                   # Project documentation
```

## ğŸš€ Workflow Chi Tiáº¿t

### Phase 1: Environment Setup (5-10 phÃºt)

#### 1.1 Prerequisites
```bash
# Kiá»ƒm tra Python version (3.8+)
python3 --version

# Kiá»ƒm tra Docker (optional)
docker --version
docker-compose --version
```

#### 1.2 Local Development Setup
```bash
# Clone repository
git clone <repository-url>
cd tichhop-git

# Táº¡o virtual environment
python3 -m venv venv

# Activate virtual environment
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Hoáº·c sá»­ dá»¥ng script setup
chmod +x scripts/setup.sh
./scripts/setup.sh
```

#### 1.3 Docker Setup (Recommended)
```bash
# Build vÃ  cháº¡y táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra status
docker-compose ps

# Xem logs
docker-compose logs -f
```

### Phase 2: Data Loading & Processing (10-15 phÃºt)

#### 2.1 Data Loading
```python
# Sá»­ dá»¥ng DataLoader
from src.etl.data_loader import DataLoader

loader = DataLoader()
raw_data = loader.load_all_sources()

# Kiá»ƒm tra dá»¯ liá»‡u
for source, df in raw_data.items():
    print(f"{source}: {len(df)} rows, {len(df.columns)} columns")
```

#### 2.2 Data Cleaning
```python
# Sá»­ dá»¥ng DataCleaner
from src.etl.data_cleaner import DataCleaner

cleaner = DataCleaner()
cleaned_data = cleaner.clean_all_data(raw_data)
standardized_data = cleaner.standardize_columns(cleaned_data)

# Xem summary cleaning
summary = cleaner.get_cleaning_summary(raw_data, standardized_data)
print(summary)
```

#### 2.3 Data Exploration
```bash
# Cháº¡y Jupyter notebook
jupyter lab notebooks/01_data_exploration.ipynb

# Hoáº·c vá»›i Docker
docker-compose exec jupyter jupyter lab --ip=0.0.0.0 --port=8888
```

### Phase 3: Analytics & Insights (15-20 phÃºt)

#### 3.1 Trend Analysis
```python
# Sá»­ dá»¥ng TrendAnalyzer
from src.analytics.trend_analyzer import TrendAnalyzer

analyzer = TrendAnalyzer()

# Combine all data
import pandas as pd
all_data = pd.concat(standardized_data.values(), ignore_index=True)

# Generate comprehensive report
trend_report = analyzer.generate_trend_report(all_data)
market_insights = analyzer.get_market_insights(all_data)

# Print insights
for insight in market_insights:
    print(f"â€¢ {insight}")
```

#### 3.2 Key Analytics Areas
- **Job Title Analysis**: Top job titles, categories, trends
- **Salary Analysis**: Salary distribution, prediction, factors
- **Geographic Analysis**: Country/city distribution, regional trends
- **Skills Analysis**: Most in-demand skills, skill clusters
- **Industry Analysis**: Industry distribution, salary by industry

### Phase 4: Dashboard & Visualization (5-10 phÃºt)

#### 4.1 Streamlit Dashboard
```bash
# Cháº¡y dashboard
streamlit run dashboard/app.py

# Hoáº·c vá»›i Docker
docker-compose exec dashboard streamlit run app.py
```

**Dashboard Features:**
- ğŸ“Š **Overview Metrics**: Total jobs, companies, locations, avg salary
- ğŸ“ˆ **Interactive Charts**: Job distribution, geographic analysis, salary trends
- ğŸ” **Data Filters**: Filter by source, job title, country, salary range
- ğŸ“‹ **Raw Data Table**: Browse and download filtered data
- ğŸ“Š **Multiple Tabs**: Job Distribution, Geographic, Salary, Trends

#### 4.2 Dashboard Navigation
1. **Sidebar Controls**: 
   - Data source selection
   - Job title filtering
   - Country/location filtering
2. **Main Content**:
   - Key metrics cards
   - Interactive visualizations
   - Data table with download option

### Phase 5: API Development (10-15 phÃºt)

#### 5.1 FastAPI Backend
```bash
# Cháº¡y API server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Hoáº·c vá»›i Docker
docker-compose exec api uvicorn main:app --host 0.0.0.0 --port 8000
```

#### 5.2 API Endpoints
```bash
# Health check
curl http://localhost:8000/health

# Get jobs with filters
curl "http://localhost:8000/api/jobs?limit=10&source=glassdoor"

# Analytics summary
curl http://localhost:8000/api/analytics/summary

# Trend analysis
curl http://localhost:8000/api/analytics/trends

# Salary prediction
curl "http://localhost:8000/api/analytics/salary-prediction?job_title=data%20scientist&location=San%20Francisco"

# Skills analysis
curl http://localhost:8000/api/analytics/skills

# Geographic analysis
curl http://localhost:8000/api/analytics/geographic
```

#### 5.3 API Documentation
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Phase 6: Database Integration (Optional - 10-15 phÃºt)

#### 6.1 PostgreSQL Setup
```bash
# Start PostgreSQL container
docker-compose up -d postgres

# Connect to database
docker-compose exec postgres psql -U admin -d job_analytics
```

#### 6.2 MongoDB Setup
```bash
# Start MongoDB container
docker-compose up -d mongodb

# Connect to MongoDB
docker-compose exec mongodb mongosh -u admin -p password123
```

## ğŸ”„ Development Workflow

### Daily Development Process

#### 1. Morning Setup (5 phÃºt)
```bash
# Activate environment
source venv/bin/activate

# Start services
docker-compose up -d

# Check status
docker-compose ps
```

#### 2. Data Processing (10-15 phÃºt)
```python
# Load and clean data
from src.etl.data_loader import DataLoader
from src.etl.data_cleaner import DataCleaner

loader = DataLoader()
cleaner = DataCleaner()

raw_data = loader.load_all_sources()
cleaned_data = cleaner.clean_all_data(raw_data)
```

#### 3. Analysis & Development (30-60 phÃºt)
```python
# Run analysis
from src.analytics.trend_analyzer import TrendAnalyzer

analyzer = TrendAnalyzer()
trend_report = analyzer.generate_trend_report(all_data)
```

#### 4. Testing & Validation (10-15 phÃºt)
```bash
# Test API endpoints
curl http://localhost:8000/health

# Test dashboard
# Open http://localhost:8501 in browser
```

#### 5. End of Day (5 phÃºt)
```bash
# Stop services
docker-compose down

# Commit changes
git add .
git commit -m "Daily updates: [description]"
git push
```

### Weekly Workflow

#### Monday: Data Quality Check
- Review data completeness
- Check for new data sources
- Update cleaning rules if needed

#### Tuesday-Thursday: Feature Development
- Implement new analytics features
- Add new visualizations
- Improve API endpoints

#### Friday: Testing & Documentation
- Run comprehensive tests
- Update documentation
- Prepare demo materials

## ğŸ› Troubleshooting

### Common Issues

#### 1. Data Loading Issues
```bash
# Check file paths
ls -la data/

# Check file permissions
chmod 644 data/*.csv

# Check file encoding
file data/DataAnalyst.csv
```

#### 2. Docker Issues
```bash
# Clean up containers
docker-compose down -v
docker system prune -f

# Rebuild containers
docker-compose build --no-cache
docker-compose up -d
```

#### 3. Memory Issues
```bash
# Check memory usage
docker stats

# Increase memory limits in docker-compose.yml
```

#### 4. Port Conflicts
```bash
# Check port usage
netstat -tulpn | grep :8000
netstat -tulpn | grep :8501

# Kill processes using ports
sudo kill -9 $(lsof -t -i:8000)
```

### Performance Optimization

#### 1. Data Processing
- Use chunking for large datasets
- Implement caching for frequent queries
- Optimize pandas operations

#### 2. API Performance
- Implement response caching
- Use async/await for I/O operations
- Add database connection pooling

#### 3. Dashboard Performance
- Lazy load visualizations
- Implement data pagination
- Use efficient chart libraries

## ğŸ“Š Monitoring & Logging

### Application Logs
```bash
# View all logs
docker-compose logs -f

# View specific service logs
docker-compose logs -f api
docker-compose logs -f dashboard
```

### Performance Monitoring
- API response times
- Memory usage
- Database query performance
- Dashboard load times

## ğŸš€ Deployment

### Production Deployment
1. **Environment Variables**: Set production configs
2. **Database Setup**: Configure production databases
3. **Security**: Implement authentication and authorization
4. **Monitoring**: Set up logging and monitoring
5. **Scaling**: Configure load balancing and auto-scaling

### CI/CD Pipeline
1. **Code Quality**: Run linting and tests
2. **Build**: Create Docker images
3. **Test**: Run integration tests
4. **Deploy**: Deploy to production
5. **Monitor**: Monitor application health

## ğŸ“ˆ Future Enhancements

### Short Term (1-2 weeks)
- [ ] Add more data sources
- [ ] Implement machine learning models
- [ ] Add real-time data updates
- [ ] Improve error handling

### Medium Term (1-2 months)
- [ ] Add user authentication
- [ ] Implement data export features
- [ ] Add more visualization types
- [ ] Create mobile app

### Long Term (3-6 months)
- [ ] Implement real-time streaming
- [ ] Add advanced ML models
- [ ] Create data marketplace
- [ ] Implement multi-tenant architecture

## ğŸ“ Support & Resources

### Documentation
- **API Docs**: http://localhost:8000/docs
- **Project README**: README.md
- **Code Comments**: Inline documentation

### Community
- **GitHub Issues**: For bug reports and feature requests
- **Discussions**: For questions and ideas
- **Wiki**: For detailed documentation

### Contact
- **Author**: Nguyá»…n Thuá»³ DÆ°Æ¡ng
- **Email**: Duong.NT252022M@sis.hust.edu.vn
- **Project**: TÃ­ch há»£p Dá»¯ liá»‡u Lá»›n - Final Project

---

*Workflow nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hÆ°á»›ng dáº«n chi tiáº¿t quÃ¡ trÃ¬nh phÃ¡t triá»ƒn vÃ  váº­n hÃ nh dá»± Ã¡n Job Market Analytics tá»« setup ban Ä‘áº§u Ä‘áº¿n deployment production.*
